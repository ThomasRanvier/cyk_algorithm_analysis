
\section{Context-free grammar}

Context-free grammars are part of the Chomsky hierarchy \cite{grammars_hierarchy, grammars} and can be defined as a quadruple $G = (N, \Sigma, P, S)$ that consists of the following components:

\begin{itemize}
    \item[$-$] $N$ is a finite set of \textit{non-terminal} variables.
    \item[$-$] $\Sigma$ is a finite set of \textit{terminal} variables.
    \item[$-$] $P$ is a finite set of \textit{production rules}, also called \textit{rewrite rules}, it specifies a symbol substitution that can be recursively performed to generate new symbol sequences. In an unrestricted grammar, which is the most general form of the Chomsky hierarchy, a production is of the form $u \to v$ where $u$ and $v$ are arbitrary strings of \textit{terminals} and \textit{non-terminals}.
    \item[$-$] $S \in N$ is a distinguished symbol that is the \textit{start symbol}, also called the \textit{sentence symbol}.
\end{itemize}

The language $L(G)$ is the language generated by the grammar G \cite{intro_automata}.
To generate a string of the language, one begins with a string consisting of only a single start symbol, and then successively applies the rules (any number of times, in any order) to rewrite this string. 
This stops when we obtain a string containing only terminals.
The language consists of all the strings that can be generated in this manner.

The same process can be applied backward to check weither a given string can be generated by the grammar or not.

\subsection{Description of the Chomsky normal-form}

Here is an example of a context-free grammar in the Chomsky normal-form:

\begin{align*} 
&S \to AB|BC\\
&A \to BA|a\\
&B \to CC|b\\
&C \to AB|a
\end{align*}

The grammar can be described as a finite set of production rules, in those productions we can find four types of symbols:

\begin{itemize}
    \item[$-$] The `$\to$' symbol is what seperates the left-hand from the right-hand(s) of the rules.
    \item[$-$] In this grammar every single uppercase letter is a \textit{non-terminal} variable.
    \item[$-$] The \textit{terminals} are primitive symbols, they can only be found on the right-hand of a production.
    \item[$-$] A left-hand is constitued of one \textit{non-terminal}, it can relate to one or several right-hands seperated by the `$|$' symbol. A right-hand is constitued either by one \textit{terminal} or a pair of \textit{non-terminals}.
\end{itemize}

Here S, A, B, and C are \textit{non-terminals}, a and b are \textit{terminals} and S is the \textit{start symbol}.

\subsection{Convert grammar to Chomsky normal-form}

Sometimes it is needed to convert a context-free grammar to the Chomsky normal-form.
Later in this report is presented an algorithm that follows the next pseudo-code in order to automaticly convert a given context-free grammar into the Chomsky normal-form.
\\
\\
For example this is how to convert the following grammar:

\begin{align*} 
&S \to S+P|P\\
&P \to P*C|C\\
&C \to (S)|0|1\\
\end{align*}

\begin{enumerate}
    \item The first thing to do is to eliminate the \textit{start symbol} from the right-hands of the grammar by replacing the \textit{start symbol} by a new one (some labels have been modified):
        \begin{align*} 
        &S \to A\\
        &A \to A+B|B\\
        &B \to B*C|C\\
        &C \to (A)|0|1\\
        \end{align*}

    \item Then the goal is to replace the \textit{terminals} that are not the only symbols on the right-hand by a new \textit{non-terminal}:
        \begin{align*} 
        &S \to A        &P \to +\\
        &A \to APB|B    &M \to *\\
        &B \to BMC|C    &L \to (\\
        &C \to LAR|0|1  &R \to )\\
        \end{align*}
    
    \item The next step is to delete the right-hands with more than 2 \textit{non-terminals} by adding \textit{non-terminals} that will relate to a pair:
        \begin{align*} 
        &S \to A        &F \to AR\\
        &A \to AD|B     &P \to +\\
        &B \to BE|C     &M \to *\\
        &C \to LF|0|1   &L \to (\\
        &D \to PB       &R \to )\\
        &E \to MC\\
        \end{align*}

    \item At last it is needed to eliminate the unit rules, those are the right-hands in which there is one \textit{non-terminal} alone:
        \begin{align*} 
        &S \to AD|BE|LG|0|1 &F \to AR\\
        &A \to AD|BE|LG|0|1 &P \to +\\
        &B \to BE|LG|0|1    &M \to *\\
        &C \to LF|0|1       &L \to (\\
        &D \to PB           &R \to )\\
        &E \to MC\\
        \end{align*}

\end{enumerate}

\section{CYK algorithm}

The CYK algorithm is a parser for context-free grammar, it is named after its inventors, John Cocke, Daniel Younger and Tadao Kasami.
The algorithm takes a string and a context-free grammar as input and determines if the string is part of the language of the grammar or not.
It uses the backward process explained in the presentation of section 1.1.

Context-free grammar parsers can be used for example in computer sciences to check code structure in compilers or in biology for DNA and RNA strings analysis.

\section{Diverse development approaches}

The goal of the assignment is to implement CYK parsers using different development approaches in order to compare the efficiency of those.
In this section the three used development methods are briefly presented.

\subsection{Divide and conquer}

Divide and conquer is an algorithmic strategy that consists in recursively breaking down a problem into several others, and do so until the problems become easy enough to solve. 
The obtained results are then combined in order to solve the initial problem.
\\
\\
Those are the steps of the general structure of that strategy:

\begin{enumerate}
    \item Divide the problem instance given as input into smaller instances.
    \item Recursively compute the results for those smaller instances.
    \item Assemble the recursively obtained results into a result for the entire input and return it.
\end{enumerate}

\subsubsection{Recurrence relation}

It is possible to find a function that satisfies the recurrence relation of the divide and conquer strategy.
Let's suppose that a recursive algorithm divides a problem of size $n$ into $a$ sub-problems, where each sub-problem is of size $\dfrac{n}{b}$
Suppose also that $g(n)$ is the total time needed to create the sub-problems and combine their results.

Then we can define $f(n)$, being the number of operations needed to solve the problem of size $n$, as:
$$
f(n) = a * f(\dfrac{n}{b}) + g(n)
$$

\subsubsection{Master theorem}

The Master theorem for the divide and conquer recurrences provides an asymptotic analysis of the complexity, using the Big O notation.
It allows one to solve $f(n)$ depending on three different cases.
\\
\\
Now that we have the function $f(n)$ we suppose that $g(n) = \Theta(n^d)$.
\\
\\
Then the master theorem is:
$$
f(n) = 
\begin{cases}
    \Theta(n^d) &\text{if } a < b^d\\
    \Theta(n^dlog(n)) &\text{if } a = b^d\\
    \Theta(n^{log_b(a)}) &\text{if } a > b^d\\
\end{cases}
$$

Here the first case is when it is possible to ignore the solving of the subproblems before the creation and combination of the sub-problems. The running time is then the time needed to create and combine the sub-problems, $g(n) = \Theta(n^d)$.

The second case is when it is not possible to ignore anything.

The final case is when it is possible to ignore the creation and combination of the sub-problems. The running time is then the time needed to solve $a^{log_b(n)}$ subproblems of size 1.

\subsection{Dynamic programing}

Dynamic programing is an algorithmic paradigm that solves a given complex problem by breaking it down into subproblems and storing the results of those subproblems to avoid computing the same results again.

\subsubsection{Top-down approach}

The top-down approach is similar to the divide and conquer approach, the initial problem is recursively broken down into several sub-problems.
The difference is that in this approach a global variable memorize every computed result. 
In that way when the program runs into a known problem the memorized result is directly returned.
The program generally computes less operations and goes less deep in the recursion than the divide and conquer method.

\subsubsection{Bottom-up approach}

The bottom-up approach consists in solving the simplest sub-problems first, memorizing every computed result, and then going up solving bigger and bigger sub-problems until it can solve the initial problem.
This approach is a way to avoid recursion, which saves some memory, but on the other hand it can sometimes solve sub-problems that will never be used to solve the initial problem, which is a waste of time.

A good analogy for the bottom-up design are building blocks, indeed you have to start assembling the most little pieces together in order to construct bigger ones that you will then assemble to get the final result.

